services:
  vllm:
    build: 
      context: .
      dockerfile:  ./Dockerfile
    entrypoint: python3 -m vllm.entrypoints.openai.api_server --model IlyaGusev/saiga_llama3_8b --dtype bfloat16 --host 0.0.0.0 --port 8000
    ports:
      - 8001:8000
    volumes:
      - cache_huggingface_volume:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu] 

volumes:
  cache_huggingface_volume:
